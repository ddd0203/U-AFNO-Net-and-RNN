import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import os
import os.path as osp
from exp import Exp  # ç›´æ¥ä½¿ç”¨ä½ çš„Expç±»
from utils import *
import argparse


class AutoregressiveFinetuner:
    """è‡ªå›å½’å¾®è°ƒå™¨ - é€‚é…å¤šæ¨¡å‹æ¶æ„"""

    def __init__(self, exp, args):
        self.exp = exp
        self.args = args
        self.device = exp.device
        self.model = exp.model
        self.criterion = exp.criterion

        # å¾®è°ƒå‚æ•°
        self.num_pred_steps = args.num_pred_steps
        self.teacher_forcing_ratio = args.teacher_forcing_ratio

        # è·å–æ•°æ®åŠ è½½å™¨
        self._get_data()

        # åˆ›å»ºå¾®è°ƒä¼˜åŒ–å™¨ - ä¸Expä¿æŒä¸€è‡´
        self._select_optimizer()

    def _get_data(self):
        """è·å–æ•°æ®åŠ è½½å™¨ - ç›´æ¥ä½¿ç”¨expçš„åŠ è½½å™¨"""
        self.train_loader = self.exp.train_loader
        self.vali_loader = self.exp.vali_loader
        self.test_loader = self.exp.test_loader

    def _select_optimizer(self):
        """é€‰æ‹©ä¼˜åŒ–å™¨ - ä¸Expç±»ä¿æŒä¸€è‡´çš„ç»“æ„"""
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=self.args.finetune_lr  # ä½¿ç”¨å¾®è°ƒå­¦ä¹ ç‡
        )
        
        # è®¡ç®—å®é™…çš„steps_per_epoch
        steps_per_epoch = len(self.train_loader)
        if hasattr(self.args, 'max_batches_per_epoch') and self.args.max_batches_per_epoch < steps_per_epoch:
            steps_per_epoch = self.args.max_batches_per_epoch
        
        # åˆ›å»ºOneCycleLRè°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer, 
            max_lr=self.args.finetune_lr,  # ä½¿ç”¨å¾®è°ƒå­¦ä¹ ç‡
            steps_per_epoch=steps_per_epoch, 
            epochs=self.args.finetune_epochs,  # ä½¿ç”¨å¾®è°ƒè½®æ•°
        )
        
        
        print_log(f"ğŸ”§ å¾®è°ƒé…ç½®:")
        print_log(f"   æ¨¡å‹ç±»å‹: {args.model_name}")
        print_log(f"   é¢„æµ‹æ­¥æ•°: {self.num_pred_steps}")
        print_log(f"   æ•™å¸ˆå¼ºåˆ¶æ¯”ä¾‹: {self.teacher_forcing_ratio}")
        print_log(f"   å¾®è°ƒå­¦ä¹ ç‡: {args.finetune_lr}")
        return self.optimizer

    def autoregressive_forward(self, inputs, targets=None, use_teacher_forcing=True):
        """
        è‡ªå›å½’å‰å‘ä¼ æ’­
        Args:
            inputs: (B, T, C, H, W) - åŸå§‹è¾“å…¥åºåˆ—
            targets: (B, N, C, H, W) - çœŸå®ç›®æ ‡åºåˆ—ï¼ˆç”¨äºæ•™å¸ˆå¼ºåˆ¶ï¼‰
            use_teacher_forcing: æ˜¯å¦ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        Returns:
            predictions: é¢„æµ‹åºåˆ—
            total_loss: æ€»æŸå¤±
        """
        B, T_in, C, H, W = inputs.shape

        predictions = []
        current_input = inputs  # (B, T_in, C, H, W)
        total_loss = 0

        for step in range(self.num_pred_steps):
            # æ¨¡å‹é¢„æµ‹ä¸‹ä¸€å¸§
            pred = self.model(current_input)  # (B, output_steps, C, H, W)
            predictions.append(pred)

            # è®¡ç®—æŸå¤±
            if targets is not None and step < targets.shape[1]:
                # ä½¿ç”¨çœŸå®ç›®æ ‡
                target = targets[:, step:step+1, :, :, :]  # (B, 1, C, H, W)
                step_loss = self.criterion(pred, target)

            total_loss = total_loss + step_loss

            # å‡†å¤‡ä¸‹ä¸€æ­¥è¾“å…¥
            if step < self.num_pred_steps - 1:
                if use_teacher_forcing and targets is not None and step < targets.shape[1] - 1:
                    if torch.rand(1).item() < self.teacher_forcing_ratio:
                        # ä½¿ç”¨çœŸå®å¸§
                        next_frame = targets[:, step:step+1, :, :, :]
                    else:
                        # ä½¿ç”¨é¢„æµ‹å¸§
                        next_frame = pred.detach()
                else:
                    # ä½¿ç”¨é¢„æµ‹å¸§
                    next_frame = pred.detach()

                # æ›´æ–°è¾“å…¥åºåˆ—ï¼šç§»é™¤æœ€è€çš„å¸§ï¼Œæ·»åŠ æ–°å¸§
                current_input = torch.cat([
                    current_input[:, 1:],  # ç§»é™¤ç¬¬ä¸€å¸§
                    next_frame  # æ·»åŠ æ–°å¸§
                ], dim=1)

        predictions = torch.cat(predictions, dim=1)  # (B, num_pred_steps, C, H, W)
        avg_loss = total_loss #/ self.num_pred_steps

        return predictions, avg_loss

    def finetune_epoch(self, dataloader, epoch):
        """å¾®è°ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = []

        pbar = tqdm(dataloader, desc=f'å¾®è°ƒ Epoch {epoch + 1}')

        for batch_idx, (batch_x, batch_y) in enumerate(pbar):
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)

            self.optimizer.zero_grad()

            # è‡ªå›å½’å‰å‘ä¼ æ’­
            predictions, loss = self.autoregressive_forward(
                batch_x,
                targets=batch_y,  # ä¼ å…¥çœŸå®ç›®æ ‡
                use_teacher_forcing=True
            )

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()
            self.scheduler.step()

            epoch_losses.append(loss.item())
            current_lr = self.optimizer.param_groups[0]['lr']
            pbar.set_postfix({'loss': f'{loss.item():.8f}',
            'lr': f'{current_lr:.2e}'})

            # é™åˆ¶æ¯ä¸ªepochçš„batchæ•°é‡
            if batch_idx >= self.args.max_batches_per_epoch:
                break

        return np.mean(epoch_losses)

    def validate_autoregressive(self, dataloader):
        """éªŒè¯è‡ªå›å½’æ€§èƒ½"""
        self.model.eval()
        val_losses = []

        with torch.no_grad():
            for batch_idx, (batch_x, batch_y) in enumerate(dataloader):
                if batch_idx >= 1000:  # é™åˆ¶éªŒè¯æ•°é‡
                    break

                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)

                # çº¯è‡ªå›å½’é¢„æµ‹ï¼ˆæ— æ•™å¸ˆå¼ºåˆ¶ï¼‰
                predictions, loss = self.autoregressive_forward(
                    batch_x,
                    targets=batch_y,
                    use_teacher_forcing=True
                )

                val_losses.append(loss.item())

        avg_loss = np.mean(val_losses)
        print_log(f"éªŒè¯æŸå¤±: {avg_loss:.8f}")

        return avg_loss

    def run_finetune(self):
        """è¿è¡Œå¾®è°ƒè¿‡ç¨‹"""
        print_log("ğŸš€ å¼€å§‹è‡ªå›å½’å¾®è°ƒ...")

        best_loss = float('inf')
        patience = 0
        max_patience = 5

        for epoch in range(self.args.finetune_epochs):
            # å¾®è°ƒè®­ç»ƒ
            train_loss = self.finetune_epoch(self.exp.train_loader, epoch)

            # éªŒè¯
            if epoch % self.args.log_step == 0:
                val_loss = self.validate_autoregressive(self.exp.vali_loader)
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = self.optimizer.param_groups[0]['lr']

                print_log(f"Epoch {epoch + 1}/{self.args.finetune_epochs}")
                print_log(f"  è®­ç»ƒæŸå¤±: {train_loss:.8f}")
                print_log(f"  éªŒè¯æŸå¤±: {val_loss:.8f}")
                print_log(f"  å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < best_loss:
                    best_loss = val_loss
                    patience = 0
                    save_path = osp.join(self.exp.checkpoints_path, 'finetune_best.pth')
                    torch.save(self.model.state_dict(), save_path)
                    print_log(f"âœ… ä¿å­˜æœ€ä½³å¾®è°ƒæ¨¡å‹: {save_path}")
                else:
                    patience += 1

                # æ—©åœ
                if patience >= max_patience:
                    print_log(f"â¹ï¸  æ—©åœè§¦å‘ï¼Œåœæ­¢å¾®è°ƒ")
                    break

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = osp.join(self.exp.checkpoints_path, 'finetune_best.pth')
        if osp.exists(best_model_path):
            self.model.load_state_dict(torch.load(best_model_path, map_location=self.device))
            print_log("âœ… åŠ è½½æœ€ä½³å¾®è°ƒæ¨¡å‹")

        return self.model


def create_finetune_parser():
    """åˆ›å»ºå¾®è°ƒå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser()

    # åŸºç¡€å‚æ•° - ä¸Expç±»ä¿æŒä¸€è‡´
    parser.add_argument('--device', default='cuda', type=str)
    parser.add_argument('--res_dir', default='./results', type=str)
    parser.add_argument('--ex_name', default='64x64_uafnop1_finetune_test_lr1e6', type=str)
    parser.add_argument('--use_gpu', default=True, type=bool)
    parser.add_argument('--gpu', default=0, type=int)
    parser.add_argument('--seed', default=1, type=int)

    # æ•°æ®å‚æ•° - ä¸Expç±»ä¿æŒä¸€è‡´
    parser.add_argument('--batch_size', default=32, type=int)
    parser.add_argument('--val_batch_size', default=32, type=int)
    parser.add_argument('--data_root', default='./data/64x64_h5new/')
    parser.add_argument('--dataname', default='custom', type=str)
    parser.add_argument('--num_workers', default=8, type=int)

    # æ¨¡å‹å‚æ•° - ä¸Expç±»ä¿æŒä¸€è‡´
    parser.add_argument('--in_shape', default=[4, 4, 64, 64], type=int, nargs='*')
    parser.add_argument('--hid_S', default=64, type=int)
    parser.add_argument('--N_T', default=8, type=int)
    parser.add_argument('--input_steps', default=4, type=int)
    parser.add_argument('--output_steps', default=1, type=int)
    parser.add_argument('--mask_path', default='./data/64x64_h5new/mask.h5', type=str)

    # æ¨¡å‹é€‰æ‹© - ä¸Expç±»ä¿æŒä¸€è‡´
    parser.add_argument('--model_name', default='compact_unet_afno',
                        choices=['unet_afno', 'compact_unet_afno', 'pure_afno', 'pure_unet'],
                        help='æ¨¡å‹ç±»å‹é€‰æ‹©')

    # AFNOå‚æ•° - ä¸Expç±»ä¿æŒä¸€è‡´
    parser.add_argument('--img_size', default=[8, 8], type=int, nargs=2)
    parser.add_argument('--patch_size', default=[1, 1], type=int, nargs=2)
    parser.add_argument('--embed_dim', default=768, type=int)
    parser.add_argument('--bilinear', default=False, type=bool)
    parser.add_argument('--mlp_ratio', default=4.0, type=float)
    parser.add_argument('--drop_rate', default=0.0, type=float)
    parser.add_argument('--drop_path_rate', default=0.0, type=float)
    parser.add_argument('--num_blocks', default=8, type=int)
    parser.add_argument('--sparsity_threshold', default=0.01, type=float)
    parser.add_argument('--hard_thresholding_fraction', default=1.0, type=float)

    # è®­ç»ƒå‚æ•° - ä¸ºäº†å…¼å®¹Expç±»
    parser.add_argument('--lr', default=1e-4, type=float, help='åŸå§‹è®­ç»ƒå­¦ä¹ ç‡(Expç±»éœ€è¦)')
    parser.add_argument('--epochs', default=20, type=int, help='åŸå§‹è®­ç»ƒè½®æ•°(Expç±»éœ€è¦)')
    parser.add_argument('--log_step', default=1, type=int)

    # å¾®è°ƒç‰¹å®šå‚æ•°
    parser.add_argument('--pretrained_model', required=True, type=str,
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--num_pred_steps', default=4, type=int,
                        help='å¾®è°ƒæ—¶é¢„æµ‹çš„æ­¥æ•°')
    parser.add_argument('--teacher_forcing_ratio', default=0.5, type=float,
                        help='æ•™å¸ˆå¼ºåˆ¶æ¯”ä¾‹ (0-1)')
    parser.add_argument('--finetune_epochs', default=15, type=int,
                        help='å¾®è°ƒè½®æ•°')
    parser.add_argument('--finetune_lr', default=1e-5, type=float,
                        help='å¾®è°ƒå­¦ä¹ ç‡')
    parser.add_argument('--max_batches_per_epoch', default=1825, type=int,
                        help='æ¯ä¸ªepochæœ€å¤§batchæ•°é‡')

    return parser


if __name__ == '__main__':
    args = create_finetune_parser().parse_args()

    print_log("ğŸ¯ å¼€å§‹è‡ªå›å½’å¾®è°ƒå®éªŒ")
    print_log(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
    print_log(f"æ¨¡å‹ç±»å‹: {args.model_name}")
    print_log(f"é¢„æµ‹æ­¥æ•°: {args.num_pred_steps}")

    try:
        # ç›´æ¥ä½¿ç”¨ä½ çš„Expç±» - å®Œå…¨å¤ç”¨æ‰€æœ‰åŠŸèƒ½
        exp = Exp(args)

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if not osp.exists(args.pretrained_model):
            raise FileNotFoundError(f"é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {args.pretrained_model}")

        # ä½¿ç”¨Expç±»çš„load_modelæ–¹æ³•
        exp.load_model(args.pretrained_model)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼ˆExpç±»å·²ç»åœ¨åˆå§‹åŒ–æ—¶æ‰“å°äº†ï¼‰
        total_params = sum(p.numel() for p in exp.model.parameters())
        print_log(f"ğŸ“Š æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,} ({total_params * 4 / 1024 ** 2:.2f} MB)")

        # åˆ›å»ºå¾®è°ƒå™¨
        finetuner = AutoregressiveFinetuner(exp, args)

        # å¾®è°ƒå‰éªŒè¯
        print_log("ğŸ“Š å¾®è°ƒå‰è‡ªå›å½’æ€§èƒ½:")
        pre_loss = finetuner.validate_autoregressive(exp.vali_loader)

        # è¿è¡Œå¾®è°ƒ
        finetuned_model = finetuner.run_finetune()

        # å¾®è°ƒåéªŒè¯
        print_log("ğŸ“Š å¾®è°ƒåè‡ªå›å½’æ€§èƒ½:")
        post_loss = finetuner.validate_autoregressive(exp.vali_loader)

        improvement = ((pre_loss - post_loss) / pre_loss * 100) if pre_loss > 0 else 0
        print_log(f"æ€§èƒ½æ”¹è¿›: {improvement:.2f}%")

        print_log("âœ… å¾®è°ƒå®Œæˆï¼")
        print_log(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {osp.join(exp.checkpoints_path, 'finetune_best.pth')}")

        # å¯é€‰ï¼šè¿›è¡Œæ ‡å‡†æµ‹è¯•è¯„ä¼°
        if hasattr(args, 'run_final_test') and args.run_final_test:
            print_log("ğŸ§ª è¿è¡Œæœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
            test_mse = exp.test(args)
            print_log(f"æœ€ç»ˆæµ‹è¯•MSE: {test_mse:.4f}")

    except Exception as e:
        print_log(f"âŒ å¾®è°ƒè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e